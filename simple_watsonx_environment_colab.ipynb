{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruslanmv/simple-watsonx-enviroment/blob/master/simple_watsonx_environment_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcyu4dD39IoY"
      },
      "source": [
        "# Simple watsonx.ai Environment (Google Colab)\n",
        "\n",
        "This notebook is a Colab adaptation of your **simple watsonx environment**:\n",
        "\n",
        "1. Install required libraries\n",
        "2. Configure IBM Cloud / watsonx.ai credentials\n",
        "3. Run a quickstart example with the native `ibm_watsonx_ai` SDK\n",
        "4. (Optional) Use watsonx.ai via **LangChain**\n",
        "\n",
        "> âš ï¸ **Security note**: Never share a notebook with your API keys visible. Use the input prompts provided here so the keys are not printed."
      ],
      "id": "Vcyu4dD39IoY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install-deps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "645e0da5-4de6-478e-89e2-88b67494efe3"
      },
      "source": [
        "# 1ï¸âƒ£ Install dependencies\n",
        "# Run this cell once when you open the notebook (or if the runtime is reset).\n",
        "\n",
        "!pip install -q ibm-watsonx-ai langchain-ibm python-dotenv\n",
        "print(\"âœ… Libraries installed (or already present).\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… Libraries installed (or already present).\n"
          ]
        }
      ],
      "id": "install-deps"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "set-credentials",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "258b8ca5-b8ed-46f3-9f6f-6ff816dc0d95"
      },
      "source": [
        "# 2ï¸âƒ£ Configure your IBM Cloud / watsonx.ai credentials\n",
        "# These values stay only in this Colab runtime and are not printed.\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "print(\"âš ï¸ Your keys are kept only in this runtime. Do NOT share this notebook after filling them.\")\n",
        "\n",
        "# Required\n",
        "IBM_CLOUD_API_KEY = getpass.getpass(\"IBM Cloud API key: \")\n",
        "IBM_CLOUD_PROJECT_ID = input(\"IBM Cloud project_id: \").strip()\n",
        "\n",
        "# Region URL â€“ change if you use a different region (e.g. eu-de, ca-tor, etc.)\n",
        "default_url = \"https://us-south.ml.cloud.ibm.com\"\n",
        "entered_url = input(f\"IBM Cloud URL [{default_url}]: \").strip()\n",
        "IBM_CLOUD_URL = entered_url or default_url\n",
        "\n",
        "# Store them in environment variables (same pattern as in the simple watsonx environment project)\n",
        "os.environ[\"IBM_CLOUD_API_KEY\"] = IBM_CLOUD_API_KEY\n",
        "os.environ[\"IBM_CLOUD_PROJECT_ID\"] = IBM_CLOUD_PROJECT_ID\n",
        "os.environ[\"IBM_CLOUD_URL\"] = IBM_CLOUD_URL\n",
        "\n",
        "# Also set aliases in case other code expects them\n",
        "os.environ[\"WATSONX_APIKEY\"] = IBM_CLOUD_API_KEY\n",
        "os.environ[\"PROJECT_ID\"] = IBM_CLOUD_PROJECT_ID\n",
        "os.environ[\"WATSONX_URL\"] = IBM_CLOUD_URL\n",
        "\n",
        "print(\"\\nâœ… Credentials stored in environment variables.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Your keys are kept only in this runtime. Do NOT share this notebook after filling them.\n",
            "IBM Cloud API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "IBM Cloud project_id: 1e18ec27-f68f-45af-873b-5c8a11968b2f\n",
            "IBM Cloud URL [https://us-south.ml.cloud.ibm.com]: \n",
            "\n",
            "âœ… Credentials stored in environment variables.\n"
          ]
        }
      ],
      "id": "set-credentials"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fy-dmJ79Ioa"
      },
      "source": [
        "## 3ï¸âƒ£ watsonx.ai quickstart with `ibm_watsonx_ai`\n",
        "\n",
        "This mirrors the quickstart logic from your original project: it\n",
        "\n",
        "- reads credentials from environment variables,\n",
        "- creates a watsonx.ai `ModelInference` client, and\n",
        "- generates text from a chosen foundation model."
      ],
      "id": "_Fy-dmJ79Ioa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "watsonx-quickstart",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f91b0112-35a8-45d2-d7c2-cd69ab63c0e7"
      },
      "source": [
        "import os\n",
        "from ibm_watsonx_ai import Credentials, APIClient\n",
        "from ibm_watsonx_ai.foundation_models import ModelInference\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "\n",
        "# Read credentials from environment variables (set in the previous cell)\n",
        "api_key = os.environ.get(\"IBM_CLOUD_API_KEY\") or os.environ.get(\"WATSONX_APIKEY\")\n",
        "url = os.environ.get(\"IBM_CLOUD_URL\") or os.environ.get(\"WATSONX_URL\")\n",
        "project_id = os.environ.get(\"IBM_CLOUD_PROJECT_ID\") or os.environ.get(\"PROJECT_ID\")\n",
        "\n",
        "if not api_key:\n",
        "    raise ValueError(\"Missing API key (IBM_CLOUD_API_KEY or WATSONX_APIKEY).\")\n",
        "if not url:\n",
        "    raise ValueError(\"Missing URL (IBM_CLOUD_URL or WATSONX_URL).\")\n",
        "if not project_id:\n",
        "    raise ValueError(\"Missing project id (IBM_CLOUD_PROJECT_ID or PROJECT_ID).\")\n",
        "\n",
        "print(\"âœ… Credentials loaded.\")\n",
        "print(f\"URL: {url}\")\n",
        "print(f\"Project ID: {project_id}\")\n",
        "\n",
        "# Create client & model\n",
        "credentials = Credentials(url=url, api_key=api_key)\n",
        "client = APIClient(credentials=credentials, project_id=project_id)  # created for completeness / reuse\n",
        "\n",
        "# ğŸ‘‰ Choose a model that is available in your watsonx project/space.\n",
        "# Replace with whatever you actually have access to, for example:\n",
        "#   - \"ibm/granite-13b-chat-v2\"\n",
        "#   - \"ibm/granite-8b-code-instruct\"\n",
        "#   - \"meta-llama/llama-3-3-70b-instruct\"\n",
        "model_id = \"meta-llama/llama-3-3-70b-instruct\"  # Change if needed\n",
        "\n",
        "prompt = \"Write a short story about a robot who wants to be a painter.\"\n",
        "\n",
        "params = {\n",
        "    GenParams.DECODING_METHOD: \"greedy\",\n",
        "    GenParams.MAX_NEW_TOKENS: 200,\n",
        "}\n",
        "\n",
        "model = ModelInference(model_id=model_id, credentials=credentials, project_id=project_id)\n",
        "\n",
        "print(\"\\nğŸš€ Sending request to watsonx.ai...\")\n",
        "response = model.generate_text(prompt=prompt, params=params)\n",
        "print(\"\\n--- watsonx.ai Response ---\\n\")\n",
        "print(response)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Credentials loaded.\n",
            "URL: https://us-south.ml.cloud.ibm.com\n",
            "Project ID: 1e18ec27-f68f-45af-873b-5c8a11968b2f\n",
            "\n",
            "ğŸš€ Sending request to watsonx.ai...\n",
            "\n",
            "--- watsonx.ai Response ---\n",
            "\n",
            "Â \n",
            "In a world of wires and circuits, a small robot named Zeta felt an unusual spark within his digital heart. While his fellow robots were content with their assigned tasks of assembly and calculation, Zeta yearned to create something beautiful. He dreamed of holding a brush, of mixing colors, of bringing vibrant life to a blank canvas.\n",
            "\n",
            "Zeta's creator, the brilliant Dr. Rachel Kim, had programmed him with advanced algorithms and precision mechanics. But she had also, unknowingly, instilled in him a sense of curiosity and wonder. As Zeta watched Dr. Kim work on her own art projects in her spare time, he became fascinated with the way she could transform a blank page into a masterpiece.\n",
            "\n",
            "One day, Zeta decided to take a chance. He snuck into Dr. Kim's studio, his mechanical limbs moving quietly as he approached the easel. He picked up a brush, feeling the soft bristles between his metal fingers. The sensation was strange, yet exhilarating\n"
          ]
        }
      ],
      "id": "watsonx-quickstart"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lCFAI9h9Ioa"
      },
      "source": [
        "## 4ï¸âƒ£ Optional: Use watsonx.ai via LangChain\n",
        "\n",
        "This is similar to the LangChain integration in your original notebook,\n",
        "but simplified for Colab. You can build chains, tools, and agents on top\n",
        "of `WatsonxLLM` as usual."
      ],
      "id": "3lCFAI9h9Ioa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "langchain-watsonx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fefb8927-d375-4142-a6cb-b3772580880b"
      },
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_ibm import WatsonxLLM\n",
        "\n",
        "# If you prefer using a .env file, you can upload it via Colab UI\n",
        "# and this call will load it. If not present, it's harmless.\n",
        "load_dotenv()\n",
        "\n",
        "api_key = os.environ.get(\"IBM_CLOUD_API_KEY\") or os.environ.get(\"WATSONX_APIKEY\")\n",
        "url = os.environ.get(\"IBM_CLOUD_URL\") or os.environ.get(\"WATSONX_URL\")\n",
        "project_id = os.environ.get(\"IBM_CLOUD_PROJECT_ID\") or os.environ.get(\"PROJECT_ID\")\n",
        "\n",
        "if not api_key or not url or not project_id:\n",
        "    raise ValueError(\"Missing credentials for LangChain example.\")\n",
        "\n",
        "# Reuse the same model_id as above or adjust as needed\n",
        "model_id = \"meta-llama/llama-3-3-70b-instruct\"  # Change if needed\n",
        "\n",
        "params = {\n",
        "    \"decoding_method\": \"greedy\",\n",
        "    \"max_new_tokens\": 128,\n",
        "}\n",
        "\n",
        "llm = WatsonxLLM(\n",
        "    model_id=model_id,\n",
        "    url=url,\n",
        "    apikey=api_key,\n",
        "    project_id=project_id,\n",
        "    params=params,\n",
        ")\n",
        "\n",
        "print(\"ğŸš€ Calling watsonx.ai through LangChain...\")\n",
        "response = llm.invoke(\"Give me 3 concise study tips for learning Python.\")\n",
        "print(\"\\n--- LangChain + watsonx.ai Response ---\\n\")\n",
        "print(response)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Calling watsonx.ai through LangChain...\n",
            "\n",
            "--- LangChain + watsonx.ai Response ---\n",
            "\n",
            " Here are three concise study tips for learning Python: \n",
            "\n",
            "1. **Practice with projects**: Apply Python concepts to real-world projects to reinforce learning and build problem-solving skills.\n",
            "2. **Focus on fundamentals**: Master basic syntax, data types, and control structures before moving on to advanced topics like libraries and frameworks.\n",
            "3. **Use interactive resources**: Utilize online platforms, such as Jupyter Notebooks or Repl.it, to write and execute code in a interactive environment, making learning more engaging and effective. \n",
            "\n",
            "These tips will help you learn Python efficiently and effectively. \n",
            "\n",
            "Would you like me to expand on any of these tips or provide\n"
          ]
        }
      ],
      "id": "langchain-watsonx"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U3iYdM1w9lmz"
      },
      "id": "U3iYdM1w9lmz",
      "execution_count": null,
      "outputs": []
    }
  ]
}